{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display_functions import display\n",
    "\n",
    "from tapestry.evaluate import evaluate_tensor_value\n",
    "from tapestry.graph_algorithms import (\n",
    "    strip_blocks,\n",
    "    shard_max_dim,\n",
    "    specialize_read_slices,\n",
    "    strip_orphan_values,\n",
    "    section_plan_max_dim,\n",
    "    expand_section_plans,\n",
    ")\n",
    "from tapestry.jupyter_utils import display_graph\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from tapestry.expression_graph import (\n",
    "    BlockOperation,\n",
    "    PinnedTensor,\n",
    "    TapestryGraph,\n",
    "    AggregateTensor,\n",
    "    TensorValue,\n",
    ")\n",
    "from tapestry.zspace import EmbeddingMode, ZRange, ZRangeMap, ZTransform, assert_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def linear_impl(\n",
    "    params: Dict[str, torch.Tensor],\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    x = params[\"input\"]\n",
    "    w = params[\"w\"]\n",
    "    bias = params.get(\"bias\")\n",
    "    return {\"result\": torch.nn.functional.linear(x, w.T, bias)}\n",
    "\n",
    "\n",
    "def linear_op(\n",
    "    *,\n",
    "    name: str = None,\n",
    "    x: TensorValue,\n",
    "    w: TensorValue,\n",
    "    bias: Optional[TensorValue] = None,\n",
    ") -> AggregateTensor:\n",
    "    graph = x.assert_graph()\n",
    "    assert w.graph == graph\n",
    "\n",
    "    assert len(w.shape) == 2, w.shape\n",
    "    in_dim = w.shape[0]\n",
    "    out_dim = w.shape[1]\n",
    "\n",
    "    assert_shape(\n",
    "        x.shape[-1:],\n",
    "        w.shape[:1],\n",
    "        \"input shape {xshape} in_dim {actual} incompatible \"\n",
    "        \"with weight shape {wshape} in_dim {expected}\",\n",
    "        xshape=x.shape,\n",
    "        wshape=w.shape,\n",
    "    )\n",
    "\n",
    "    index_space = ZRange(x.shape[:-1].tolist() + [out_dim])\n",
    "\n",
    "    op = graph.add_node(\n",
    "        BlockOperation(\n",
    "            name=name,\n",
    "            operation=\"linear\",\n",
    "            index_space=index_space,\n",
    "            compute_cost=ZTransform(\n",
    "                projection=np.ones((index_space.ndim, 1)),\n",
    "            ),\n",
    "            memory_cost=ZTransform(\n",
    "                projection=np.ones((index_space.ndim, 1)),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    op.bind_input(\n",
    "        name=\"input\",\n",
    "        value=x,\n",
    "        selector=ZRangeMap(\n",
    "            transform=ZTransform(\n",
    "                projection=[\n",
    "                    [1, 0],\n",
    "                    [0, 0],\n",
    "                ],\n",
    "            ),\n",
    "            shape=[1, in_dim],\n",
    "        ).embed(op.index_space.ndim, mode=EmbeddingMode.TILE),\n",
    "    )\n",
    "\n",
    "    projection = np.zeros((index_space.ndim, 2))\n",
    "    projection[-1, -1] = 1\n",
    "\n",
    "    op.bind_input(\n",
    "        name=\"w\",\n",
    "        value=w,\n",
    "        selector=ZRangeMap(\n",
    "            transform=ZTransform(\n",
    "                projection=projection,\n",
    "            ),\n",
    "            shape=[in_dim, 1],\n",
    "        ).embed(op.index_space.ndim, mode=EmbeddingMode.CLIP),\n",
    "    )\n",
    "\n",
    "    if bias is not None:\n",
    "        assert_shape(\n",
    "            bias.shape,\n",
    "            w.shape[-1:],\n",
    "            \"bias shape {actual} != weight [out_dim] {expected}\",\n",
    "        )\n",
    "\n",
    "        op.bind_input(\n",
    "            name=\"bias\",\n",
    "            value=bias,\n",
    "            selector=ZRangeMap(\n",
    "                transform=ZTransform(projection=[[0], [1]]),\n",
    "                shape=[1],\n",
    "            ).embed(op.index_space.ndim, mode=EmbeddingMode.CLIP),\n",
    "        )\n",
    "\n",
    "    result = op.bind_result(\n",
    "        name=\"result\",\n",
    "        selector=ZRangeMap(\n",
    "            transform=ZTransform(\n",
    "                projection=[[1, 0], [0, 1]],\n",
    "            ),\n",
    "            shape=[1, 1],\n",
    "        ).embed(op.index_space.ndim, mode=EmbeddingMode.TILE),\n",
    "        dtype=x.dtype,\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def relu_impl(\n",
    "    params: Dict[str, torch.Tensor],\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    x = params[\"input\"]\n",
    "    return {\"result\": torch.nn.functional.relu(x)}\n",
    "\n",
    "\n",
    "def relu_op(\n",
    "    value: TensorValue,\n",
    ") -> AggregateTensor:\n",
    "    graph = value.assert_graph()\n",
    "\n",
    "    index_space = ZRange(value.shape)\n",
    "\n",
    "    op = graph.add_node(\n",
    "        BlockOperation(\n",
    "            operation=\"relu\",\n",
    "            index_space=index_space,\n",
    "            compute_cost=ZTransform(\n",
    "                projection=np.ones((index_space.ndim, 1)),\n",
    "            ),\n",
    "            memory_cost=ZTransform(\n",
    "                projection=np.zeros((index_space.ndim, 1)),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    selector = ZRangeMap.identity_map().embed(\n",
    "        op.index_space.ndim,\n",
    "        mode=EmbeddingMode.TILE,\n",
    "    )\n",
    "\n",
    "    op.bind_input(\n",
    "        name=\"input\",\n",
    "        value=value,\n",
    "        selector=selector,\n",
    "    )\n",
    "\n",
    "    result = op.bind_result(\n",
    "        name=\"result\",\n",
    "        selector=selector,\n",
    "        dtype=value.dtype,\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_example_graph():\n",
    "    g = TapestryGraph()\n",
    "\n",
    "    x = g.add_node(\n",
    "        PinnedTensor(\n",
    "            name=\"X\",\n",
    "            shape=[20, 30, 128],\n",
    "            dtype=torch.float64,\n",
    "            storage=\"store:x\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    w1 = g.add_node(\n",
    "        PinnedTensor(\n",
    "            name=\"W1\",\n",
    "            shape=[128, 32],\n",
    "            dtype=torch.float64,\n",
    "            storage=\"store:w1\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    b1 = g.add_node(\n",
    "        PinnedTensor(\n",
    "            name=\"B1\",\n",
    "            shape=[32],\n",
    "            dtype=torch.float64,\n",
    "            storage=\"store:b1\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    a = linear_op(name=\"L1\", x=x, w=w1, bias=b1)\n",
    "\n",
    "    y = relu_op(a)\n",
    "\n",
    "    w2 = g.add_node(\n",
    "        PinnedTensor(\n",
    "            name=\"W2\",\n",
    "            shape=[32, 8],\n",
    "            dtype=torch.float64,\n",
    "            storage=\"store:w\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    z = relu_op(linear_op(name=\"L2\", x=y, w=w2))\n",
    "\n",
    "    g.mark_observed(z)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "g = build_example_graph()\n",
    "\n",
    "# TODO: better node path / name lookup semantics.\n",
    "# Q: are names unique? grouped? here are errors thrown?\n",
    "# _we_ have unique IDs; might be easier to require exeternal maps.\n",
    "l1 = g.list_nodes(BlockOperation, filter=lambda n: n.name == \"L1\")[0]\n",
    "\n",
    "# demonstrate manual sectioning.\n",
    "l1.attach_section_plan([1, 2, 2])\n",
    "\n",
    "display_graph(g)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sectioned = g.clone()\n",
    "section_plan_max_dim(sectioned, 2)\n",
    "display_graph(sectioned)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sharded = sectioned.clone()\n",
    "expand_section_plans(sharded)\n",
    "specialize_read_slices(sharded)\n",
    "display_graph(sharded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shard_only = sharded.clone()\n",
    "strip_blocks(shard_only)\n",
    "strip_orphan_values(shard_only)\n",
    "display_graph(shard_only)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tapestry import evaluate\n",
    "\n",
    "env = evaluate.Environment()\n",
    "for pinned in shard_only.list_nodes(PinnedTensor):\n",
    "    env.values[pinned.node_id] = torch.rand(tuple(pinned.shape), dtype=pinned.dtype)\n",
    "\n",
    "env.operations = {\n",
    "    \"linear\": linear_impl,\n",
    "    \"relu\": relu_impl,\n",
    "}\n",
    "\n",
    "obs = {node_id: shard_only.get_node(node_id) for node_id in sharded.observed}\n",
    "vals = {}\n",
    "\n",
    "for node in obs.values():\n",
    "    vals[node.node_id] = evaluate_tensor_value(node, env=env)\n",
    "\n",
    "display(vals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}